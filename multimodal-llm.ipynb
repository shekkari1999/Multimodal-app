{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Overview**\n",
    "\n",
    "The restaurant aggregator industry is becoming increasingly reliant on sophisticated technology solutions to stay competitive in the fast-paced food delivery market. In this context, providing users with highly personalized and contextually relevant food recommendations is crucial for enhancing user experience and fostering brand loyalty. Traditional recommendation systems, predominantly based on textual data, often fail to capture the comprehensive and nuanced preferences of users, especially when it comes to visual appeal and presentation of dishes. A multimodal approach that incorporates both text and images can vastly improve the accuracy and personalization of recommendations.\n",
    "\n",
    "This project aims to design and implement a Multimodal Retrieval-Augmented Generation (RAG) system for a restaurant aggregator app, enhancing its capability to deliver precise food recommendations tailored to individual preferences and dietary needs. The system will integrate and process multiple data types—textual descriptions and visual content—from restaurants to generate personalized suggestions. Utilizing technologies such as Amazon S3 for data storage, Amazon Bedrock for image summarization, and FAISS for efficient similarity search, the system will encode and retrieve vectorized data representations. A user interface powered by Streamlit will facilitate interactive and user-friendly querying, ultimately leading to dynamic and context-aware recommendation generation. \n",
    "\n",
    "![image](reference-images/notebook/delivery-app.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Approach**\n",
    "\n",
    "* Data Reading and Preprocessing:\n",
    "    * Data Storage: Utilize Amazon S3 for scalable and reliable object storage of the collected data and read the images and metadata.\n",
    "\n",
    "* Data Processing:\n",
    "    * Image Data: Use Anthropic Claude-Sonnet model to generate image descriptions \n",
    "\n",
    "* Vector Database:\n",
    "    * Storage and Retrieval: Use Amazon Titan Embeddings and FAISS for storing and efficiently retrieving vectorized data, enabling fast and scalable search capabilities for the recommendation engine.\n",
    "\n",
    "* Recommendation Engine: \n",
    "    * Utilizing Anthropic Claude Sonnet Multimodal Model: To create conversational chatbot and generate recommendations from Vector DB results.\n",
    "\n",
    "* Streamlit API Development and Integration:\n",
    "    * User Interface: Develop an interactive user interface using Streamlit, which will serve the frontend for users to interact with the system.\n",
    "    * Functionalities: Users will be able to interact with the chatbot to search text or image inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Solution Architecture**\n",
    "\n",
    "![image](reference-images/notebook/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning Outcomes**\n",
    "\n",
    "* Learn the fundamentals of multimodal data (text, images) for LLM applications\n",
    "* Gain insights into how Amazon S3, Amazon Bedrock, and FAISS can be utilized in recommendation system applications\n",
    "* Learn how to use S3 for efficient, scalable, and secure data storage.\n",
    "* Understand how to preprocess text and image data for large language model applications\n",
    "* Explore the capabilities of AWS Titan for transforming text data into embeddings\n",
    "* Understand the functionality of vector databases in search and retrieval systems\n",
    "* Learn how to use FAISS for efficient storage and retrieval of high-dimensional data vectors\n",
    "* Learn about prompt engineering techniques and result optimization techniques for RAG applications\n",
    "* Learn the steps to develop a recommendation system that can suggest items based on user preferences using a multimodal LLM \n",
    "* Develop skills in using Langchain and Streamlit to create interactive, user-friendly chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prerequisites**\n",
    "\n",
    "* Technical Familiarity: Participants should have a basic understanding of Python programming, machine learning concepts, and experience with APIs and cloud platforms like AWS.\n",
    "\n",
    "* Data Preparation Skills: Knowledge of data collection, preprocessing, and the ability to work with different data formats (text, images) is crucial.\n",
    "\n",
    "* Tool Proficiency: A little familiarity with specific tools and libraries such as Streamlit, FAISS, and machine learning frameworks could help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multimodal Data**\n",
    "\n",
    "Multimodal data refers to information collected in various forms, including text, images, audio, video, and even sensor data, which are integrated and analyzed together to enhance the performance of artificial intelligence (AI) systems. This integration allows AI models to process and understand complex scenarios more like humans do, by interpreting different types of data concurrently. The significance of multimodal data in real-world applications is profound, enabling more nuanced and context-aware AI solutions. For example, in healthcare, multimodal data can combine medical images, patient records, and doctor's audio notes to improve diagnostic accuracy. In customer service, text from chat interactions, voice data from calls, and video data can be analyzed together to enhance response quality and service personalization. The ability to merge and interpret this varied data leads to richer insights and more effective AI applications across diverse fields.\n",
    "\n",
    "![image](reference-images/notebook/modalities.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Retreival Augmented Generation**\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a sophisticated approach that combines the capabilities of retrieval systems with generative models to enhance the performance and applicability of AI in tasks requiring detailed knowledge or context. This methodology involves two primary steps: retrieving relevant data from a large database or corpus, and then using a generative model to synthesize responses based on the retrieved data.\n",
    "\n",
    "### **How RAG Works:**\n",
    "Retrieval Phase: In the first phase, the system identifies and retrieves the most relevant documents or pieces of information from a dataset. This is usually achieved through vector-based search techniques where documents are converted into high-dimensional vectors using models like BERT or other embeddings. These vectors are then indexed using systems such as FAISS (Facebook AI Similarity Search) to enable rapid retrieval.\n",
    "\n",
    "Augmentation Phase: After retrieval, the generative component comes into play. This could be a language model that takes the retrieved documents as additional context or \"knowledge\" to generate responses. The model effectively integrates this information, ensuring that the output is both relevant and contextually rich.\n",
    "\n",
    "### **Benefits of RAG:**\n",
    "\n",
    "**Improved Accuracy:** By leveraging the strengths of both retrieval and generation, RAG models can help achieve accuracy and make LLMs avoid hallucination.\n",
    "By using relevant documents as a source of truth, RAG models can provide more accurate and detailed responses than purely generative models.\n",
    "\n",
    "**Scalability:** Retrieval systems can efficiently handle large databases, making RAG scalable for enterprise-level applications.\n",
    "\n",
    "\n",
    "![image](reference-images/notebook/jumpstart-fm-rag.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Important Execution Instructions**\n",
    "\n",
    "For detailed execution instructions, including how to set up your environment, configure AWS credentials, and deploy the application on an EC2 instance, please refer to the [README file](./readme.md).\n",
    "\n",
    "### **Steps Covered in the README:**\n",
    "1. **Requesting Model Access on AWS Bedrock:** Steps to gain access to necessary models on AWS.\n",
    "2. **Data Setup on S3:** Instructions on creating an S3 bucket and uploading the data.\n",
    "3. **Virtual Environment Creation:** Detailed guide for setting up the Python environment.\n",
    "4. **AWS CLI - Credentials Setup:** How to configure your AWS credentials.\n",
    "5. **Streamlit Deployment on EC2 (Optional):** Guide to deploying the Streamlit application on an EC2 instance.\n",
    "\n",
    "Make sure to follow the README file for any setup or deployment steps before running the code in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from utils import *\n",
    "import base64\n",
    "import os\n",
    "from io import StringIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How to Fetch Data from S3 Bucket**\n",
    "\n",
    "To fetch data from a CSV file stored in an Amazon S3 bucket using boto3 and convert it into a Pandas DataFrame, you'll first need to set up your Python environment and AWS credentials. boto3 is the Amazon Web Services (AWS) SDK for Python, which allows Python developers to write software that makes use of Amazon services like S3. You'll also need pandas, a powerful data manipulation and analysis library in Python, to handle the CSV data once it is retrieved. Start by installing the required libraries with pip install boto3 pandas and ensure your AWS credentials are configured, either by using the AWS CLI to set them globally or by manually setting environment variables.\n",
    "\n",
    "Once your environment is set up, you can begin by creating an S3 client using boto3, which allows you to interact with the S3 service. This involves specifying your AWS region and optionally including your access keys if they are not set globally. Using this client, you can call the get_object method to fetch the desired CSV file from your S3 bucket. This method requires the name of the bucket and the file key, which is the path to the file within the bucket. The response from get_object will include the file content in binary format, which can then be decoded into a string. This binary-to-string conversion is necessary for processing the CSV content with Pandas.\n",
    "\n",
    "To convert the CSV data into a Pandas DataFrame, you need to use the io.StringIO class, which creates an in-memory stream for text I/O. This class is used to treat the string data as a file-like object, allowing Pandas to read it using pd.read_csv. Once the data is loaded into a DataFrame, you can utilize Pandas' robust data manipulation capabilities to analyze and process the data as needed. This approach enables seamless integration of AWS S3 and Pandas, providing a scalable solution for handling large datasets stored in the cloud. With these steps, you can efficiently fetch, load, and analyze data directly from Amazon S3, leveraging Python's extensive data processing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a session using Amazon S3\n",
    "s3_client = boto3.client('s3', region_name='your-region', \n",
    "                         aws_access_key_id='your-access-key-id', \n",
    "                         aws_secret_access_key='your-secret-access-key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3', region_name='ap-south-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_csv_from_s3(bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Fetches a CSV file from S3 and converts it into a Pandas DataFrame.\n",
    "    \n",
    "    :param bucket_name: Name of the S3 bucket\n",
    "    :param file_key: Key (path) to the CSV file in the bucket\n",
    "    :return: DataFrame containing the CSV data\n",
    "    \"\"\"\n",
    "    # Fetch the CSV file from S3\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    \n",
    "    # Read the CSV file content\n",
    "    csv_content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # Use StringIO to convert the CSV string into a file-like object\n",
    "    csv_buffer = StringIO(csv_content)\n",
    "    \n",
    "    # Load the CSV data into a DataFrame\n",
    "    df = pd.read_csv(csv_buffer)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_id</th>\n",
       "      <th>restaurant_name</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>menu_item_id</th>\n",
       "      <th>menu_item_name</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>protein</th>\n",
       "      <th>carbs</th>\n",
       "      <th>fats</th>\n",
       "      <th>calories</th>\n",
       "      <th>dietary_warnings</th>\n",
       "      <th>vegetarian_or_nonveg</th>\n",
       "      <th>image_path</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>price</th>\n",
       "      <th>serves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R001</td>\n",
       "      <td>La Bella Italia</td>\n",
       "      <td>Italian</td>\n",
       "      <td>R001M001</td>\n",
       "      <td>Margherita Pizza</td>\n",
       "      <td>tomatoes, mozzarella cheese, basil, olive oil,...</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>images/R001/R001M001.png</td>\n",
       "      <td>4.5</td>\n",
       "      <td>12</td>\n",
       "      <td>1-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R001</td>\n",
       "      <td>La Bella Italia</td>\n",
       "      <td>Italian</td>\n",
       "      <td>R001M002</td>\n",
       "      <td>Spaghetti Carbonara</td>\n",
       "      <td>spaghetti, eggs, cheese, pancetta, black pepper</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>400</td>\n",
       "      <td>Contains eggs, Contains dairy</td>\n",
       "      <td>Non-Vegetarian</td>\n",
       "      <td>images/R001/R001M002.png</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R001</td>\n",
       "      <td>La Bella Italia</td>\n",
       "      <td>Italian</td>\n",
       "      <td>R001M003</td>\n",
       "      <td>Lasagna</td>\n",
       "      <td>pasta sheets, ground beef, ricotta cheese, moz...</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>450</td>\n",
       "      <td>Contains dairy</td>\n",
       "      <td>Non-Vegetarian</td>\n",
       "      <td>images/R001/R001M003.png</td>\n",
       "      <td>4.6</td>\n",
       "      <td>16</td>\n",
       "      <td>1-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R001</td>\n",
       "      <td>La Bella Italia</td>\n",
       "      <td>Italian</td>\n",
       "      <td>R001M004</td>\n",
       "      <td>Bruschetta</td>\n",
       "      <td>bread, tomatoes, garlic, basil, olive oil</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>images/R001/R001M004.png</td>\n",
       "      <td>3.8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R001</td>\n",
       "      <td>La Bella Italia</td>\n",
       "      <td>Italian</td>\n",
       "      <td>R001M005</td>\n",
       "      <td>Tiramisu</td>\n",
       "      <td>ladyfingers, coffee, mascarpone cheese, cocoa ...</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>300</td>\n",
       "      <td>Contains dairy, Contains eggs</td>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>images/R001/R001M005.png</td>\n",
       "      <td>3.1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  restaurant_id  restaurant_name  cuisine menu_item_id       menu_item_name  \\\n",
       "0          R001  La Bella Italia  Italian     R001M001     Margherita Pizza   \n",
       "1          R001  La Bella Italia  Italian     R001M002  Spaghetti Carbonara   \n",
       "2          R001  La Bella Italia  Italian     R001M003              Lasagna   \n",
       "3          R001  La Bella Italia  Italian     R001M004           Bruschetta   \n",
       "4          R001  La Bella Italia  Italian     R001M005             Tiramisu   \n",
       "\n",
       "                                         ingredients  protein  carbs  fats  \\\n",
       "0  tomatoes, mozzarella cheese, basil, olive oil,...       12     30    15   \n",
       "1    spaghetti, eggs, cheese, pancetta, black pepper       18     40    20   \n",
       "2  pasta sheets, ground beef, ricotta cheese, moz...       25     35    22   \n",
       "3          bread, tomatoes, garlic, basil, olive oil        4     15     5   \n",
       "4  ladyfingers, coffee, mascarpone cheese, cocoa ...        6     25    15   \n",
       "\n",
       "   calories               dietary_warnings vegetarian_or_nonveg  \\\n",
       "0       350                            NaN           Vegetarian   \n",
       "1       400  Contains eggs, Contains dairy       Non-Vegetarian   \n",
       "2       450                 Contains dairy       Non-Vegetarian   \n",
       "3       120                            NaN           Vegetarian   \n",
       "4       300  Contains dairy, Contains eggs           Vegetarian   \n",
       "\n",
       "                 image_path  average_rating  price serves  \n",
       "0  images/R001/R001M001.png             4.5     12    1-2  \n",
       "1  images/R001/R001M002.png             4.0     10    1-2  \n",
       "2  images/R001/R001M003.png             4.6     16    1-2  \n",
       "3  images/R001/R001M004.png             3.8      8      1  \n",
       "4  images/R001/R001M005.png             3.1     12      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usage\n",
    "bucket_name = 'multimodal-food-recommendation'\n",
    "file_key = 'restaurants_menu_data.csv'\n",
    "\n",
    "df = fetch_csv_from_s3(bucket_name, file_key)\n",
    "\n",
    "# use this code if reading data from local folder\n",
    "# df = pd.read_csv(\"data/restaurants_menu_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating Bedrock Runtime**\n",
    "\n",
    "The provided code snippet demonstrates the initialization of two objects from the langchain_community library: BedrockChat and BedrockEmbeddings. These classes are used to interface with AI models for natural language processing tasks. The BedrockChat object is configured to interact with a language model, specifically \"anthropic.claude-3-sonnet-20240229-v1:0\", via the Bedrock service, with parameters such as a maximum token limit of 2048, a temperature setting of 0.0 for deterministic outputs, and a stop sequence indicating where the model should halt its response. \n",
    "\n",
    "The BedrockEmbeddings object is also initialized using the Bedrock service, utilizing the model \"amazon.titan-embed-text-v2:0\" for generating text embeddings, which are vector representations of text that can be used in various machine learning applications, such as semantic similarity tasks or clustering. This setup facilitates the integration of advanced language models into applications that require conversational AI and text processing capabilities, leveraging the power of pre-trained models provided by the Bedrock service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "bedrock = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "\n",
    "model_kwargs =  {\n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0.0,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}\n",
    "\n",
    "llm = BedrockChat(\n",
    "    client=bedrock,\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "\n",
    "embeddings=BedrockEmbeddings(\n",
    "    client=bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Why and How to Encode Images in Base64 for Use with Large Language Models (LLMs)**\n",
    "\n",
    "Images are inherently binary data, meaning they consist of a sequence of bytes that represent pixel values. However, many systems and protocols (like JSON, HTML, or various APIs) are text-based and designed to handle text rather than binary data. Directly embedding binary data in these systems can cause issues, such as data corruption or unexpected errors, because binary data might include control characters that are interpreted incorrectly.\n",
    "\n",
    "\n",
    "Base64 is a text-based encoding scheme that converts binary data into a string of ASCII characters. This encoding uses 64 characters (hence the name) consisting of uppercase and lowercase letters, digits, and two additional symbols (+ and /). These characters are universally supported in text-based formats, making Base64 a safe and reliable way to represent binary data as text.\n",
    "\n",
    "Large Language Models (LLMs) when extended to multimodal models, are designed to process and generate not just text but also other forms of data, such as images. By encoding images in Base64, they can be embedded directly into text prompts or inputs, enabling the model to process both the text and the associated image data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_from_s3(bucket_name, image_path):\n",
    "    \"\"\"\n",
    "    Fetches an image from S3 and encodes it in base64.\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    :param image_path: The relative path to the image in the S3 bucket.\n",
    "    :return: The base64-encoded string of the image.\n",
    "    \"\"\"\n",
    "    # Fetch the image from S3\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=image_path)\n",
    "    \n",
    "    # Read the image content as binary\n",
    "    image_content = response['Body'].read()\n",
    "\n",
    "    # Encode the image content to a base64 string\n",
    "    encoded_image = base64.b64encode(image_content).decode('utf-8')\n",
    "    \n",
    "    return encoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function if you are using images from local folder instead of S3 bucket\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode('utf-8')\n",
    "    \n",
    "# df['encoded_image'] = df['image_path'].apply(encode_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'encoded_image' by applying the encode_image_from_s3 function\n",
    "df['encoded_image'] = df['image_path'].apply(lambda x: encode_image_from_s3(bucket_name, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_id</th>\n",
       "      <th>restaurant_name</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>menu_item_id</th>\n",
       "      <th>menu_item_name</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>protein</th>\n",
       "      <th>carbs</th>\n",
       "      <th>fats</th>\n",
       "      <th>calories</th>\n",
       "      <th>dietary_warnings</th>\n",
       "      <th>vegetarian_or_nonveg</th>\n",
       "      <th>image_path</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>price</th>\n",
       "      <th>serves</th>\n",
       "      <th>encoded_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R001</td>\n",
       "      <td>La Bella Italia</td>\n",
       "      <td>Italian</td>\n",
       "      <td>R001M001</td>\n",
       "      <td>Margherita Pizza</td>\n",
       "      <td>tomatoes, mozzarella cheese, basil, olive oil,...</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>images/R001/R001M001.png</td>\n",
       "      <td>4.5</td>\n",
       "      <td>12</td>\n",
       "      <td>1-2</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAABD0AAALJCAYAAAC3J1hNAA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  restaurant_id  restaurant_name  cuisine menu_item_id    menu_item_name  \\\n",
       "0          R001  La Bella Italia  Italian     R001M001  Margherita Pizza   \n",
       "\n",
       "                                         ingredients  protein  carbs  fats  \\\n",
       "0  tomatoes, mozzarella cheese, basil, olive oil,...       12     30    15   \n",
       "\n",
       "   calories dietary_warnings vegetarian_or_nonveg                image_path  \\\n",
       "0       350              NaN           Vegetarian  images/R001/R001M001.png   \n",
       "\n",
       "   average_rating  price serves  \\\n",
       "0             4.5     12    1-2   \n",
       "\n",
       "                                       encoded_image  \n",
       "0  iVBORw0KGgoAAAANSUhEUgAABD0AAALJCAYAAAC3J1hNAA...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understanding Human and System Messages in LangChain for Enhanced Prompt Structuring**\n",
    "\n",
    "When working with Large Language Models (LLMs) through frameworks like LangChain, managing the way you structure prompts is crucial for obtaining relevant and accurate responses from the model. LangChain introduces abstractions like HumanMessage and SystemMessage to help organize and improve the interaction between users and the model. These message types are fundamental in building effective prompts, particularly for chat-based models, where the input isn't just a simple string but a sequence of messages.\n",
    "\n",
    "What are Human and System Messages?\n",
    "HumanMessage: This represents the input or query provided by the user to the model. It is essentially the user's contribution to the conversation or task. For example, if you ask the model, \"What is the weather today?\", this input is captured as a HumanMessage.\n",
    "\n",
    "SystemMessage: This message type is used to set the context, guidelines, or rules that the model should follow during the interaction. It’s not part of the user's query but rather serves to influence the behavior of the model. For example, a SystemMessage might state, \"You are a weather assistant that provides accurate and up-to-date weather information.\"\n",
    "\n",
    "These messages help structure the interaction in a way that separates user inputs from system-level instructions, allowing the model to understand what to prioritize and how to behave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert encode image data into text we'll use our Multimodal LLM to generate descriptions of the image. We are converting all data into one fromat - Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are providing image name as initial context (this is optional, either way the model should  be able to detect dish and generate summary) to the model to generate relevant summaries and build a robust rag system\n",
    "def describe_image(encoded_image, image_name):\n",
    "    \n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are an AI assistant specializing in analyzing and describing food images. Your task is to provide a concise and accurate description of the food item.\"),\n",
    "        HumanMessage(content=[\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\"\"You are an assistant tasked with providing detailed descriptions of the dish {image_name} in the image. Your descriptions should focus exclusively on the food and its ingredients, without mentioning any non-food items such as plates, utensils, or decorations. Follow these guidelines to create a detailed and accurate description in a short paragraph:\n",
    "\n",
    "\n",
    "Describe the appearance of the dish:\n",
    "Provide a vivid and savory description of how the dish looks, including colors, textures, and presentation.\n",
    "\n",
    "Cuisine and taste experience:\n",
    "Specify the cuisine of the dish and describe how it feels to eat, including taste, aroma, and overall mouthfeel.\n",
    "\n",
    "Ingredients:\n",
    "List the key ingredients used in the dish, emphasizing fresh and distinctive components.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                },\n",
    "            },\n",
    "        ])\n",
    "    ]\n",
    "\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row\n",
    "df['image_description'] = df.apply(lambda row: describe_image(row['encoded_image'], row['menu_item_name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_id</th>\n",
       "      <th>restaurant_name</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>menu_item_id</th>\n",
       "      <th>menu_item_name</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>protein</th>\n",
       "      <th>carbs</th>\n",
       "      <th>fats</th>\n",
       "      <th>calories</th>\n",
       "      <th>dietary_warnings</th>\n",
       "      <th>vegetarian_or_nonveg</th>\n",
       "      <th>image_path</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>price</th>\n",
       "      <th>serves</th>\n",
       "      <th>encoded_image</th>\n",
       "      <th>image_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R001</td>\n",
       "      <td>La Bella Italia</td>\n",
       "      <td>Italian</td>\n",
       "      <td>R001M001</td>\n",
       "      <td>Margherita Pizza</td>\n",
       "      <td>tomatoes, mozzarella cheese, basil, olive oil,...</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>images/R001/R001M001.png</td>\n",
       "      <td>4.5</td>\n",
       "      <td>12</td>\n",
       "      <td>1-2</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAABD0AAALJCAYAAAC3J1hNAA...</td>\n",
       "      <td>This classic Margherita pizza showcases vibran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  restaurant_id  restaurant_name  cuisine menu_item_id    menu_item_name  \\\n",
       "0          R001  La Bella Italia  Italian     R001M001  Margherita Pizza   \n",
       "\n",
       "                                         ingredients  protein  carbs  fats  \\\n",
       "0  tomatoes, mozzarella cheese, basil, olive oil,...       12     30    15   \n",
       "\n",
       "   calories dietary_warnings vegetarian_or_nonveg                image_path  \\\n",
       "0       350              NaN           Vegetarian  images/R001/R001M001.png   \n",
       "\n",
       "   average_rating  price serves  \\\n",
       "0             4.5     12    1-2   \n",
       "\n",
       "                                       encoded_image  \\\n",
       "0  iVBORw0KGgoAAAANSUhEUgAABD0AAALJCAYAAAC3J1hNAA...   \n",
       "\n",
       "                                   image_description  \n",
       "0  This classic Margherita pizza showcases vibran...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to s3://multimodal-food-recommendation/menu_descriptions_data.csv\n"
     ]
    }
   ],
   "source": [
    "# To avoid rerunning of LLMs and creating summaries again, we are going to store our updated df in s3 \n",
    "# We can fetch it when we are rerunning the codes / experimenting further\n",
    "def save_df_to_s3(df, bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame as a CSV file in an S3 bucket.\n",
    "    \n",
    "    :param df: The DataFrame to be saved.\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    :param file_key: The S3 key (path) where the CSV will be saved.\n",
    "    \"\"\"\n",
    "    # Convert DataFrame to CSV in memory\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    \n",
    "    # Upload the CSV to S3\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=csv_buffer.getvalue())\n",
    "    print(f\"DataFrame saved to s3://{bucket_name}/{file_key}\")\n",
    "\n",
    "\n",
    "file_key = 'menu_descriptions_data.csv'\n",
    "\n",
    "save_df_to_s3(df, bucket_name, file_key)\n",
    "\n",
    "\n",
    "# You can use the below code to save in local directory\n",
    "# df.to_csv(\"data/menu_descriptions_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quiz - 1**\n",
    "\n",
    "\n",
    "Test your knowledge with our first quiz!\n",
    "\n",
    "[Start Quiz 1](https://forms.gle/FvXz4eQGKspFvQfo7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This classic Margherita pizza showcases vibrant colors and rustic textures. The crust is golden-brown with charred blistered edges, providing a pleasing crunch. Melted pools of fresh mozzarella cheese mingle with bright red tomato sauce, dotted with basil leaves. The aroma hints at garlic, olive oil, and the sweet fragrance of tomatoes and herbs. Each bite delivers a harmonious blend of flavors - the tangy tomato sauce, creamy cheese, and herbaceous basil create a delightfully balanced taste experience that captures the essence of traditional Neapolitan pizza. The key ingredients are a simple yet flavorful combination of crushed tomatoes, fresh mozzarella, basil, and a perfectly baked pizza dough.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['image_description'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 18 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   restaurant_id         50 non-null     object \n",
      " 1   restaurant_name       50 non-null     object \n",
      " 2   cuisine               50 non-null     object \n",
      " 3   menu_item_id          50 non-null     object \n",
      " 4   menu_item_name        50 non-null     object \n",
      " 5   ingredients           50 non-null     object \n",
      " 6   protein               50 non-null     int64  \n",
      " 7   carbs                 50 non-null     int64  \n",
      " 8   fats                  50 non-null     int64  \n",
      " 9   calories              50 non-null     int64  \n",
      " 10  dietary_warnings      30 non-null     object \n",
      " 11  vegetarian_or_nonveg  50 non-null     object \n",
      " 12  image_path            50 non-null     object \n",
      " 13  average_rating        50 non-null     float64\n",
      " 14  price                 50 non-null     int64  \n",
      " 15  serves                50 non-null     object \n",
      " 16  encoded_image         50 non-null     object \n",
      " 17  image_description     50 non-null     object \n",
      "dtypes: float64(1), int64(5), object(12)\n",
      "memory usage: 7.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dietary_warnings'] = df['dietary_warnings'].fillna(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create full description by combining image description and metadata of the menu item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_description'] = df.apply(lambda row: f\"{row['image_description']}, Ingredients: {row['ingredients']}, \"\n",
    "                                               f\"Protein: {row['protein']}g, Carbs: {row['carbs']}g, Fats: {row['fats']}g, \"\n",
    "                                               f\"Calories: {row['calories']}, Dietary Warnings: {row['dietary_warnings']}, \"\n",
    "                                               f\"Type: {row['vegetarian_or_nonveg']}, Rating: {row['average_rating']}, \"\n",
    "                                               f\"Price: {row['price']}, Serves: {row['serves']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['restaurant_id', 'restaurant_name', 'cuisine', 'menu_item_id',\n",
       "       'menu_item_name', 'ingredients', 'protein', 'carbs', 'fats', 'calories',\n",
       "       'dietary_warnings', 'vegetarian_or_nonveg', 'image_path',\n",
       "       'average_rating', 'price', 'serves', 'encoded_image',\n",
       "       'image_description', 'full_description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quiz - 1**\n",
    "\n",
    "\n",
    "Did you finish the first Quiz yet? If not, complete it to test your knowledge?\n",
    "\n",
    "\n",
    "[Start Quiz 1](https://forms.gle/FvXz4eQGKspFvQfo7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating Structured Document Objects in LangChain**\n",
    "\n",
    "Let's break down the concepts behind the code that creates Document objects from a DataFrame using LangChain. This is particularly useful when you need to organize data into a structured format that can be easily processed by vector databases.\n",
    "\n",
    "What is LangChain's Document Class?\n",
    "The Document class in LangChain is a schema that helps encapsulate textual content along with its metadata. This structure is essential when dealing with large datasets where each text element (like a document, article, or description) needs to be tagged with additional information, such as identifiers, categories, or attributes.\n",
    "\n",
    "LangChain's Document class provides a way to create these structured text objects, which can then be processed by various models, such as those used for document retrieval, or recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# Initialize an empty list to store the Document objects\n",
    "documents = []\n",
    "\n",
    "# Iterate over each row in the DataFrame 'df'\n",
    "for idx, row in df.iterrows():\n",
    "    \n",
    "    # Create a Document object for each row\n",
    "    doc = Document(\n",
    "        # Set the main content of the document to the 'full_description' column\n",
    "        page_content=row['full_description'],\n",
    "        \n",
    "        # Add additional metadata to the document\n",
    "        metadata={\n",
    "            'id': row['menu_item_id'],                  # Unique ID for the menu item\n",
    "            'type': 'image',                            # Type of content, in this case, an image\n",
    "            'name':  row['menu_item_name'],             # Name of the menu item\n",
    "            'image_path': row['image_path'],            # Path to the associated image\n",
    "            'restaurant_name': row['restaurant_name'],  # Name of the restaurant\n",
    "            'cuisine': row['cuisine'],                  # Type of cuisine\n",
    "            'menu_item_name': row['menu_item_name'],    # Name of the menu item\n",
    "            'ingredients': row['ingredients'],          # List of ingredients\n",
    "            'nutrition': f\"Protein: {row['protein']}g, Carbs: {row['carbs']}g, Fats: {row['fats']}g \", # Nutritional info\n",
    "            'calories': row['calories'],                # Caloric content of the item\n",
    "            'dietary_warnings': row['dietary_warnings'],# Any dietary warnings (e.g., allergens)\n",
    "            'vegetarian': row['vegetarian_or_nonveg'],  # Whether the item is vegetarian or non-vegetarian\n",
    "            'average_rating': row['average_rating'],    # Average customer rating\n",
    "            'price': row['price'],                      # Price of the menu item\n",
    "            'serves': row['serves']                     # Number of servings per item\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Append the created Document object to the documents list\n",
    "    documents.append(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if all the data is included in the full_description, creating separate metadata fields is important because it allows for efficient data retrieval and searching, as structured metadata can be quickly queried without parsing unstructured text. It ensures consistency and integrity, making it easier to validate and manage data, especially in large datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a FAISS vector store from the documents and embeddings\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "# Saving the FAISS vector store locally\n",
    "vectorstore.save_local(\"output/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the FAISS vector store from local storage\n",
    "db = FAISS.load_local(\"output/faiss_index\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test how good is the similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: The Italian sub is a mouthwatering culinary delight. The crusty golden-brown bread roll is generously stuffed with layers of vibrant greens, juicy tomatoes, and succulent slices of cured meats. The meats are draped in a creamy, tangy sauce that oozes out, inviting each bite. Crisp pickled peppers add a zesty punch, while fresh lettuce and tomatoes lend a refreshing crunch. This iconic Italian-American sandwich tantalizes the senses with its harmonious blend of savory meats, creamy dressings, and crisp vegetables, creating an explosion of flavors in every satisfying bite. The aroma of herbs and spices wafts through the air, whetting the appetite for this indulgent and satisfying handheld meal., Ingredients: salami, ham, pepperoni, provolone, lettuce, tomato, Italian dressing, hoagie roll, Protein: 22g, Carbs: 35g, Fats: 25g, Calories: 480, Dietary Warnings: Contains gluten, dairy, Type: Non-Vegetarian, Rating: 4.6, Price: 10, Serves: 1, Metadata: {'id': 'R009M004', 'type': 'image', 'name': 'Italian Sub', 'image_path': 'images/R009/R009M004.png', 'restaurant_name': 'Sandwich Corner', 'cuisine': 'Sandwiches', 'menu_item_name': 'Italian Sub', 'ingredients': 'salami, ham, pepperoni, provolone, lettuce, tomato, Italian dressing, hoagie roll', 'nutrition': 'Protein: 22g, Carbs: 35g, Fats: 25g ', 'calories': 480, 'dietary_warnings': 'Contains gluten, dairy', 'vegetarian': 'Non-Vegetarian', 'average_rating': 4.6, 'price': 10, 'serves': '1'}, Score: 1.2159464359283447\n",
      "Content: This classic Margherita pizza showcases vibrant colors and rustic textures. The crust is golden-brown with charred blistered edges, providing a pleasing crunch. Melted pools of fresh mozzarella cheese mingle with bright red tomato sauce, dotted with basil leaves. The aroma hints at garlic, olive oil, and the sweet fragrance of tomatoes and herbs. Each bite delivers a harmonious blend of flavors - the tangy tomato sauce, creamy cheese, and herbaceous basil create a delightfully balanced taste experience that captures the essence of traditional Neapolitan pizza. The key ingredients are a simple yet flavorful combination of crushed tomatoes, fresh mozzarella, basil, and a perfectly baked pizza dough., Ingredients: tomatoes, mozzarella cheese, basil, olive oil, flour, yeast, Protein: 12g, Carbs: 30g, Fats: 15g, Calories: 350, Dietary Warnings:  , Type: Vegetarian, Rating: 4.5, Price: 12, Serves: 1-2, Metadata: {'id': 'R001M001', 'type': 'image', 'name': 'Margherita Pizza', 'image_path': 'images/R001/R001M001.png', 'restaurant_name': 'La Bella Italia', 'cuisine': 'Italian', 'menu_item_name': 'Margherita Pizza', 'ingredients': 'tomatoes, mozzarella cheese, basil, olive oil, flour, yeast', 'nutrition': 'Protein: 12g, Carbs: 30g, Fats: 15g ', 'calories': 350, 'dietary_warnings': ' ', 'vegetarian': 'Vegetarian', 'average_rating': 4.5, 'price': 12, 'serves': '1-2'}, Score: 1.2866419553756714\n",
      "Content: The classic Margherita pizza presents a vibrant and appetizing display. The thin, lightly charred crust provides a crisp foundation for the fresh toppings. Bright red tomato sauce is spread evenly across the surface, punctuated by pools of melted, creamy mozzarella cheese. Verdant green basil leaves add a pop of color and herbaceous aroma. The simple combination of quality ingredients creates a harmonious blend of flavors - the tangy tomatoes, rich cheese, and fragrant basil meld together in each savory bite. This quintessential Neapolitan dish offers a delightfully rustic yet refined taste experience, capturing the essence of traditional Italian cuisine with its straightforward and satisfying preparation., Ingredients: tomatoes, mozzarella cheese, basil, olive oil, flour, yeast, Protein: 12g, Carbs: 30g, Fats: 15g, Calories: 350, Dietary Warnings: Contains gluten, dairy, Type: Vegetarian, Rating: 4.5, Price: 12, Serves: 1-2, Metadata: {'id': 'R006M001', 'type': 'image', 'name': 'Margherita Pizza', 'image_path': 'images/R006/R006M001.png', 'restaurant_name': 'Pizza Paradise', 'cuisine': 'Pizza', 'menu_item_name': 'Margherita Pizza', 'ingredients': 'tomatoes, mozzarella cheese, basil, olive oil, flour, yeast', 'nutrition': 'Protein: 12g, Carbs: 30g, Fats: 15g ', 'calories': 350, 'dietary_warnings': 'Contains gluten, dairy', 'vegetarian': 'Vegetarian', 'average_rating': 4.5, 'price': 12, 'serves': '1-2'}, Score: 1.296775460243225\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = db.similarity_search_with_score(\"italian dishes\", k=3)\n",
    "\n",
    "for doc, score in relevant_docs:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: This classic Chinese sweet and sour pork dish presents a vibrant array of colors and textures. Succulent pieces of battered and fried pork are coated in a glossy, tangy-sweet sauce that glistens under the light. The pork morsels exhibit a crispy golden-brown exterior giving way to tender, juicy meat inside. Complementing the pork are vibrant bell peppers, their red, green, and yellow hues adding pops of color and crunch. Chunks of pineapple provide bursts of tropical sweetness. The dish exudes an enticing aroma of garlic, ginger, and vinegar mingling with the caramelized sugars of the sauce. Each bite delivers a harmonious balance of sweet, sour, and savory flavors, with the crispy pork providing a satisfying textural contrast to the tender vegetables and juicy pineapple., Ingredients: pork, pineapple, bell peppers, vinegar, sugar, Protein: 18g, Carbs: 20g, Fats: 15g, Calories: 350, Dietary Warnings:  , Type: Non-Vegetarian, Rating: 2.9, Price: 15, Serves: 1-2, Metadata: {'id': 'R002M002', 'type': 'image', 'name': 'Sweet and Sour Pork', 'image_path': 'images/R002/R002M002.png', 'restaurant_name': 'Dragon Palace', 'cuisine': 'Chinese', 'menu_item_name': 'Sweet and Sour Pork', 'ingredients': 'pork, pineapple, bell peppers, vinegar, sugar', 'nutrition': 'Protein: 18g, Carbs: 20g, Fats: 15g ', 'calories': 350, 'dietary_warnings': ' ', 'vegetarian': 'Non-Vegetarian', 'average_rating': 2.9, 'price': 15, 'serves': '1-2'}, Score: 1.4176688194274902\n",
      "Content: The image showcases the classic South Indian dish Vada Sambar. The vadas, or savory lentil donuts, are a deep golden brown hue with a crispy exterior and fluffy interior texture. The sambar, a lentil-based vegetable stew, has a vibrant orange-red color and appears richly spiced. Accompanying the vadas and sambar is a creamy coconut chutney flecked with green herbs, providing a cool contrast to the spicy sambar.\n",
      "\n",
      "This quintessential vegetarian meal from the Tamil Nadu region offers an explosion of flavors and aromas. The vadas deliver a satisfying crunch that gives way to a soft, savory filling. The sambar broth is infused with a medley of spices like cumin, coriander, and chili peppers, creating a complex and comforting taste. The chutney adds a refreshing coconut note to balance the heat.\n",
      "\n",
      "Key ingredients include lentils, rice, curry leaves, tamarind, coconut, and an array of aromatic spices like mustard seeds, cumin, and chilies, all combining to create this iconic and flavorful South Indian dish., Ingredients: urad dal, curry leaves, coconut chutney, tamarind, Protein: 12g, Carbs: 30g, Fats: 15g, Calories: 350, Dietary Warnings:  , Type: Vegetarian, Rating: 4.6, Price: 2, Serves: 1-2, Metadata: {'id': 'R010M003', 'type': 'image', 'name': 'Vada Sambar', 'image_path': 'images/R010/R010M003.png', 'restaurant_name': 'Dakshin Delights', 'cuisine': 'South Indian', 'menu_item_name': 'Vada Sambar', 'ingredients': 'urad dal, curry leaves, coconut chutney, tamarind', 'nutrition': 'Protein: 12g, Carbs: 30g, Fats: 15g ', 'calories': 350, 'dietary_warnings': ' ', 'vegetarian': 'Vegetarian', 'average_rating': 4.6, 'price': 2, 'serves': '1-2'}, Score: 1.5232948064804077\n",
      "Content: This classic Kung Pao Chicken dish presents a vibrant array of colors and textures. Tender chunks of chicken are coated in a savory, slightly sweet and tangy sauce, with a hint of heat from chili peppers. Bright red bell peppers and green onions add a fresh crunch, complementing the velvety sauce that glistens on the caramelized chicken pieces. Roasted peanuts provide a welcome nutty contrast. This iconic Sichuan cuisine dish tantalizes the senses with its bold flavors, aromatic spices like garlic and ginger, and the perfect balance of sweet, sour, and spicy notes in every bite. Key ingredients include diced chicken, soy sauce, rice vinegar, sesame oil, chili peppers or chili garlic sauce, bell peppers, green onions, and roasted peanuts., Ingredients: chicken, peanuts, bell peppers, chili peppers, soy sauce, Protein: 20g, Carbs: 10g, Fats: 18g, Calories: 300, Dietary Warnings: Contains peanuts, Type: Non-Vegetarian, Rating: 4.6, Price: 8, Serves: 1-2, Metadata: {'id': 'R002M001', 'type': 'image', 'name': 'Kung Pao Chicken', 'image_path': 'images/R002/R002M001.png', 'restaurant_name': 'Dragon Palace', 'cuisine': 'Chinese', 'menu_item_name': 'Kung Pao Chicken', 'ingredients': 'chicken, peanuts, bell peppers, chili peppers, soy sauce', 'nutrition': 'Protein: 20g, Carbs: 10g, Fats: 18g ', 'calories': 300, 'dietary_warnings': 'Contains peanuts', 'vegetarian': 'Non-Vegetarian', 'average_rating': 4.6, 'price': 8, 'serves': '1-2'}, Score: 1.567731261253357\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = db.similarity_search_with_score(\"sweet dishes\", k=3)\n",
    "\n",
    "for doc, score in relevant_docs:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, when I search for sweet dishes, the first response it is giving me is a chinese sweet and sour pork dish.\n",
    "Our objective is to improve the search system to build a better RAG system. Let's see how we can do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Some techniques to use to improve Similarity Search**\n",
    "\n",
    "Aligning queries and documents in the semantic space is a crucial process for improving the accuracy and relevance of search results. This alignment ensures that the user's intent is better matched with the content of the documents, even when the query lacks precise language or semantic richness. Here are some techniques used for aligning queries and documents:\n",
    "\n",
    "### **Query Rewriting**:\n",
    "This technique involves transforming the original user query into a more semantically rich or contextually appropriate query. Techniques like HyDE (Hypothetical Document Embeddings) generate hypothetical answers to enhance the query, which can then be used to retrieve more relevant documents.\n",
    "\n",
    "\n",
    "### **Hypothetical Document Embeddings**\n",
    "\n",
    "HyDE prompting, short for Hypothetical Document Embeddings, is a technique used in AI and LLM applications to enhance search accuracy and efficiency. The core idea behind HyDE is to generate a hypothetical document based on a user's query using a language model. This generated document, while not necessarily accurate, contains patterns and context that can be embedded into a vector space. These embeddings are then used to retrieve similar documents from a trusted knowledge base.\n",
    "\n",
    "This method is particularly useful in situations where the original query might not have a direct match in the database or when the query is too vague. By generating a hypothetical answer first, HyDE allows the system to leverage the semantic understanding of the language model to find relevant documents, thus improving the quality of the search results. It also mitigates the risk of hallucinations in responses by ensuring that the final answers are based on actual, reliable documents from the database. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is designed to enhance a user's search query by leveraging the Hypothetical Document Embeddings (HyDE) technique, specifically within the context of culinary recommendations. The goal is to refine the search process by generating a more effective query that aligns with the user's preferences while avoiding overly specific or irrelevant results.\n",
    "\n",
    "Query Rewriting and HyDE\n",
    "Query Rewriting is a technique that involves transforming or refining the user's original input into a more effective query that can yield better search results. This is particularly useful when the initial query is vague, incomplete, or could lead to suboptimal search outcomes. In the context of our code, query rewriting is applied to generate search terms that are more likely to match relevant dishes in your database, even if the user's input is not highly specific.\n",
    "\n",
    "HyDE (Hypothetical Document Embeddings) is a method where a model generates a hypothetical document (or query, in this case) based on the user's input. This document is then used to retrieve relevant information from a database. **The challenge with HyDE in our case is that if the generated hypothetical results are too specific, they might not match any existing entries in your database, leading to missed opportunities for relevant results. Example Italian dishes - LLM can generate top 3 dishes which are not in database, and similarity search will give random results.**\n",
    "\n",
    "How HyDE is Applied in the Code?\n",
    "In this code, a modified version of HyDE or Query Rewriting is used, but with a focus on generating relevant or synonymoud key search terms rather than detailed, specific dish recommendations or specific documents. The reasoning behind this approach is that generating highly specific hypothetical results could cause the system to miss relevant dishes that are slightly different from the user's original query but still suitable.\n",
    "\n",
    "\n",
    "**Why Only Keywords?**\n",
    "By generating only keywords, the system avoids the pitfall of creating too specific hypothetical recommendations that might not exist in the database. For instance, if a user requests a \"spicy vegan tofu stir-fry with low sodium,\" and the database does not have an exact match, a highly specific query might return no results. However, by focusing on key terms like \"vegan,\" \"tofu,\" and \"stir-fry,\" the system can find related dishes that match most of the user's preferences, even if not perfectly.\n",
    "\n",
    "This approach strikes a balance between using Query Rewriting to enhance the search process and ensuring that the generated query remains broad enough to retrieve relevant results from the existing database. The goal is to improve the likelihood of finding a satisfactory match, even if the initial user input was not fully aligned with the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_search(user_input):\n",
    "\n",
    "    hyde_prompt = [\n",
    "            SystemMessage(content=\"You are an expert culinary assistant. Your task is to produce a search query description based on user input or preference.\"),\n",
    "            HumanMessage(content=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f'''You are an expert culinary assistant tasked with generating a search query that helps recommends a variety of menu items based on user preferences. \n",
    "                    User Input:\n",
    "\n",
    "                    {user_input}\n",
    "\n",
    "                    Generate a Response That Includes Just the Key Unique Search Terms according to the user's preference, do not include unnecessary words that don't help search.\n",
    "                    The search query may or may not contain the following parameters. For example you can include similar menu items as per the user preference if mentioned, if preferences is mentioned enhance and give key search terms based on preferences.\n",
    "                    The goal is to either create a detailed query using specific information provided by the user or enhance the input to find similar preferences when the information is vague.\n",
    "                    \n",
    "                    Menu Items:\n",
    "\n",
    "                    List different dishes or food items that resemble the user's input.\n",
    "                    Mention their respective cuisines.\n",
    "\n",
    "                    Cuisines:\n",
    "\n",
    "                    Include a variety of cuisines that may match or complement the user's preferences.\n",
    "\n",
    "                    Descriptions and Ingredients:\n",
    "                    Provide a very short description of each dish.\n",
    "                    List key ingredients for each dish.\n",
    "\n",
    "                    Dietary Preferences:\n",
    "\n",
    "                    Add any dietary preferences mentioned by the user, such as vegetarian, non-vegetarian, vegan, etc.\n",
    "\n",
    "                    Nutritional Information:\n",
    "\n",
    "                    Add important nutritional preference mentioned by the user if any such as high protein, number of calories, etc.\n",
    "                    Mention serving sizes.\n",
    "                    Dietary Warnings and Suggestions:\n",
    "\n",
    "                    Avoid any dishes or ingredients containing any allergen mentioned by the user if any suggest menu items without these, and ensure all recommended items are free from this allergen.\n",
    "\n",
    "    '''}])]\n",
    "    response = llm.invoke(hyde_prompt)\n",
    "\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_search_query = enhance_search(\"dishes with high protein and low calories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple code to clean the enhanced search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes the input text.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: str, the text to clean.\n",
    "    \n",
    "    Returns:\n",
    "    - str, the cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Replace newline and tab characters with a space\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'high protein low calorie dishes lean proteins grilled proteins proteinrich salads veggie protein bowls tofu dishes lentil dishes egg white dishes greek yogurt dishes cottage cheese dishes cuisines mediterranean mexican indian thai american italian grilled chicken salad grilled chicken breast mixed greens tomatoes cucumber lowfat dressing mediterranean key ingredients chicken breast lettuce tomatoes cucumber tofu stirfry firm tofu mixed veggies lowsodium soy sauce asian key ingredients tofu vegetables soy sauce lentil soup lentils veggies herbs broth mediterranean indian key ingredients lentils vegetables broth egg white omelet egg whites veggies lowfat cheese american key ingredients egg whites vegetables cheese greek yogurt parfait greek yogurt fresh berries nuts mediterranean key ingredients greek yogurt berries nuts dietary preferences vegetarian nonvegetarian nutritional information high protein low calorie appropriate serving sizes'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(enhanced_search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the enhanced search query is more relevant to find similar items from our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: The image showcases a delectable tempura dish from Japanese cuisine. Golden-brown shrimp pieces are lightly battered and fried to crispy perfection, their delicate crunch giving way to tender, succulent bites. Drizzled with a creamy white sauce, the tempura shrimp exudes an irresistible aroma of savory seasonings and fragrant oil. Alongside the shrimp lies a vibrant green salad, providing a fresh contrast to the richness of the tempura. Key ingredients include plump shrimp, light tempura batter, vegetable oil for frying, and likely a dipping sauce like tentsuyu made with dashi broth, soy sauce, and mirin. Each bite promises an explosion of flavors and textures that epitomize the art of Japanese frying., Ingredients: shrimp, vegetables, flour, egg, panko breadcrumbs, Protein: 10g, Carbs: 15g, Fats: 10g, Calories: 180, Dietary Warnings: Contains eggs, Type: Non-Vegetarian, Rating: 2.7, Price: 12, Serves: 1-2, Metadata: {'id': 'R004M003', 'type': 'image', 'name': 'Tempura', 'image_path': 'images/R004/R004M003.png', 'restaurant_name': 'Sakura Sushi', 'cuisine': 'Japanese', 'menu_item_name': 'Tempura', 'ingredients': 'shrimp, vegetables, flour, egg, panko breadcrumbs', 'nutrition': 'Protein: 10g, Carbs: 15g, Fats: 10g ', 'calories': 180, 'dietary_warnings': 'Contains eggs', 'vegetarian': 'Non-Vegetarian', 'average_rating': 2.7, 'price': 12, 'serves': '1-2'}\n",
      "Content: Miso soup is a warm, comforting Japanese dish with a rich, savory broth. The golden-hued liquid has a velvety texture and is topped with soft cubes of silken tofu that gently float on the surface. Vibrant green slices of scallion add a fresh, slightly pungent aroma and contrasting color. The soup is garnished with a sprinkling of bright red grated ginger, providing a zesty kick to the umami-rich flavors. This nourishing and delicately balanced soup combines the earthy essence of fermented miso paste with the delicate sweetness of dashi broth, creating a harmonious blend of flavors that tantalize the taste buds with each soothing spoonful., Ingredients: tofu, seaweed, green onions, miso paste, Protein: 5g, Carbs: 6g, Fats: 2g, Calories: 70, Dietary Warnings:  , Type: Vegetarian, Rating: 4.4, Price: 10, Serves: 1, Metadata: {'id': 'R004M004', 'type': 'image', 'name': 'Miso Soup', 'image_path': 'images/R004/R004M004.png', 'restaurant_name': 'Sakura Sushi', 'cuisine': 'Japanese', 'menu_item_name': 'Miso Soup', 'ingredients': 'tofu, seaweed, green onions, miso paste', 'nutrition': 'Protein: 5g, Carbs: 6g, Fats: 2g ', 'calories': 70, 'dietary_warnings': ' ', 'vegetarian': 'Vegetarian', 'average_rating': 4.4, 'price': 10, 'serves': '1'}\n",
      "Content: This veggie sandwich presents a vibrant array of colors and textures. The crusty golden-brown bread encases a generous filling of grilled vegetables, their charred edges adding depth of flavor. Bright red roasted bell peppers, tender eggplant slices, and fresh leafy greens create an enticing medley. The sandwich exudes an aroma of toasted bread mingling with the earthy notes of the roasted veggies. Each bite delivers a satisfying crunch followed by a burst of flavors - the sweetness of the peppers, the slight bitterness of the greens, and the smoky char from the grill. This vegetarian delight offers a harmonious blend of tastes and textures, making for a delightfully filling and flavorful meal., Ingredients: lettuce, tomato, cucumbers, peppers, hummus, whole grain bread, Protein: 10g, Carbs: 40g, Fats: 5g, Calories: 250, Dietary Warnings: Contains gluten, Type: Vegetarian, Rating: 4.4, Price: 7, Serves: 1, Metadata: {'id': 'R009M002', 'type': 'image', 'name': 'Veggie Sandwich', 'image_path': 'images/R009/R009M002.png', 'restaurant_name': 'Sandwich Corner', 'cuisine': 'Sandwiches', 'menu_item_name': 'Veggie Sandwich', 'ingredients': 'lettuce, tomato, cucumbers, peppers, hummus, whole grain bread', 'nutrition': 'Protein: 10g, Carbs: 40g, Fats: 5g ', 'calories': 250, 'dietary_warnings': 'Contains gluten', 'vegetarian': 'Vegetarian', 'average_rating': 4.4, 'price': 7, 'serves': '1'}\n",
      "Content: The enchiladas present a vibrant and appetizing display of Mexican cuisine. Soft, warm corn tortillas are stuffed with a savory filling, likely shredded chicken or beef simmered in a rich, smoky sauce. The enchiladas are topped with a bright green tomatillo salsa, adding a tangy and slightly acidic contrast to the richness underneath. Shredded cheese melts over the top, creating pockets of gooey deliciousness. Freshly chopped red onion and cilantro provide pops of color, aroma, and freshness. Served alongside are wedges of lime to squeeze over the enchiladas, enhancing the flavors with a burst of citrus. This dish promises a delightful interplay of textures and flavors - the tender tortillas, hearty protein filling, vibrant salsas, and creamy cheese coming together in each satisfying bite., Ingredients: tortillas, chicken, enchilada sauce, cheese, Protein: 20g, Carbs: 30g, Fats: 15g, Calories: 380, Dietary Warnings: Contains dairy, Type: Non-Vegetarian, Rating: 4.8, Price: 5, Serves: 1, Metadata: {'id': 'R005M005', 'type': 'image', 'name': 'Enchiladas', 'image_path': 'images/R005/R005M005.png', 'restaurant_name': 'Fiesta Mexicana', 'cuisine': 'Mexican', 'menu_item_name': 'Enchiladas', 'ingredients': 'tortillas, chicken, enchilada sauce, cheese', 'nutrition': 'Protein: 20g, Carbs: 30g, Fats: 15g ', 'calories': 380, 'dietary_warnings': 'Contains dairy', 'vegetarian': 'Non-Vegetarian', 'average_rating': 4.8, 'price': 5, 'serves': '1'}\n",
      "Content: This classic Chinese-American dish presents a vibrant array of colors and textures. Tender strips of beef are coated in a rich, glossy brown sauce, its savory aroma hinting at soy, garlic, and ginger flavors. The beef is stir-fried with crisp-tender broccoli florets in a brilliant green hue. Bright red bell peppers and yellow bell peppers add pops of color and crunch. The dish promises a harmonious blend of umami depth from the sauce, contrasted by the fresh vegetal notes of the broccoli and peppers. Key ingredients include marinated beef, broccoli, bell peppers, garlic, ginger, soy sauce, and likely cornstarch to thicken the luscious sauce coating the ingredients., Ingredients: beef, broccoli, soy sauce, garlic, Protein: 22g, Carbs: 12g, Fats: 10g, Calories: 320, Dietary Warnings:  , Type: Non-Vegetarian, Rating: 3.2, Price: 10, Serves: 1-2, Metadata: {'id': 'R002M004', 'type': 'image', 'name': 'Beef and Broccoli', 'image_path': 'images/R002/R002M004.png', 'restaurant_name': 'Dragon Palace', 'cuisine': 'Chinese', 'menu_item_name': 'Beef and Broccoli', 'ingredients': 'beef, broccoli, soy sauce, garlic', 'nutrition': 'Protein: 22g, Carbs: 12g, Fats: 10g ', 'calories': 320, 'dietary_warnings': ' ', 'vegetarian': 'Non-Vegetarian', 'average_rating': 3.2, 'price': 10, 'serves': '1-2'}\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = db.similarity_search(enhanced_search_query, k=5)\n",
    "\n",
    "for doc in relevant_docs:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have gotten satisfactory results, now let's move on and understand our chatbot flow with streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quiz - 2**\n",
    "\n",
    "\n",
    "Test your knowledge with our second quiz!\n",
    "\n",
    "[Start Quiz 2](https://forms.gle/bE6jhuiGK67VvJUV8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recommendation System Chatbot**\n",
    "\n",
    "\n",
    "The design of the chatbot in this Streamlit app revolves around creating a seamless and interactive food recommendation experience. The approach combines **text and image inputs** to enhance the user's search query, leveraging a conversational interface that dynamically updates based on user interactions.\n",
    "\n",
    "### **Chat State Management**\n",
    "The chatbot uses Streamlit's session state to maintain the conversation context throughout the user session. This includes tracking the user’s past inputs, generated responses from the assistant, and any images uploaded by the user. By storing this information in the session state, the chatbot can maintain a coherent conversation, allowing the user to see both their queries and the assistant’s responses in a consistent format.\n",
    "\n",
    "### **Input Processing**\n",
    "The app accepts both textual and image inputs. When an image is uploaded, it is encoded and described using a language model, which then enhances the user’s text input by incorporating this description. This enriched input is used to generate a search query that is more likely to yield relevant results. This approach ensures that the chatbot can handle various forms of user input, making the interaction more flexible and responsive to different user needs.\n",
    "\n",
    "### **Query Enhancement and Response Generation**\n",
    "The core of the chatbot’s functionality is centered around enhancing the user's search query through a method that borrows from the concept of query rewriting. Instead of generating highly specific hypothetical dishes, which could lead to irrelevant results if the exact dish isn’t in the database, the chatbot focuses on generating key search terms. This increases the likelihood of finding relevant matches in the database, even if they aren't exact matches to the user's initial input.\n",
    "\n",
    "### **Interaction Flow**\n",
    "Once the input is processed and the search query is enhanced, the app performs a similarity search against a pre-built FAISS index. The results from this search are compiled into a context, which is then fed into a language model to generate a response. Depending on whether a recommendation is requested, the chatbot either provides a list of similar dishes (with accompanying images) or a general response based on the search results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevance Checker function judges if the search result is relevant to user or not, if not we avoid showcasing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relevance_checker(context, preference, llm):\n",
    "\n",
    "    relevance_prompt = [\n",
    "                SystemMessage(content=\"You are a restaurant assistant specializing in helping customers find the food they want.\"),\n",
    "                HumanMessage(content=[\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f'''Answer the question \"Is this dish relevant to the user by comparing dish details and user preference?\" in one word either Yes or No, based only on the following context:\n",
    "                        {context}\n",
    "                        User Preference: {preference}\n",
    "                        Answer:'''}])]\n",
    "    response = llm.invoke(relevance_prompt)\n",
    "\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context = '''\n",
    "This classic Margherita pizza showcases vibrant colors and rustic textures. The crust is golden-brown with charred blistered edges, providing a pleasing crunch. Melted pools of fresh mozzarella cheese mingle with bright red tomato sauce, dotted with basil leaves. The aroma hints at garlic, olive oil, and yeasty dough. Each bite delivers a harmonious blend of flavors - the tangy tomatoes, creamy cheese, fragrant basil, and chewy yet crisp crust creating an authentic Neapolitan pizza experience that tantalizes the senses with its simplicity and freshness. The key ingredients are a crispy hand-stretched dough, San Marzano tomatoes, fresh buffalo mozzarella, basil leaves, and a drizzle of olive oil., Ingredients: tomatoes, mozzarella cheese, basil, olive oil, flour, yeast, Protein: 12g, Carbs: 30g, Fats: 15g, Calories: 350, Dietary Warnings: nan, Type: Vegetarian, Rating: 4.5\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '''south indian dish'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_checker(context, user_input, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also include a dish summary function which gives out a summary while reasoning why the recommendation is perfect as per user input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if user input is italian dish, then the context is perfect and it should pass relevance checker, now dish summary function should give a quick summary and reason why it is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '''italian dish'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dish_summary(dish_description, preference, llm):\n",
    "\n",
    "\n",
    "    summary_prompt = [\n",
    "                SystemMessage(content=\"You are a culinary assistant designed to summarize the dish description in accordance with the user preference.\"),\n",
    "                HumanMessage(content=[\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f'''\n",
    " Your task is to create a very short two lines summary of the dish in a savoury manner by highlighting the user preference. The summary should suggest why the dish is perfect for the user as per their preference.\n",
    " The summary should include dish name, origin, ingredients and any other relevant information requested by the user in a friendly way. Do not include unnecessary sentences or additional comments like here is your response. Just give the summry description.\n",
    "\n",
    "\n",
    "            Dish Description:\n",
    "\n",
    "            {dish_description} \n",
    "            \n",
    "            User Preference:\n",
    "\n",
    "            {preference}\n",
    "'''}])]\n",
    "    response = llm.invoke(summary_prompt)\n",
    "\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Margherita Pizza - A Neapolitan Delight\\nHailing from Italy, this classic showcases the vibrant flavors of San Marzano tomatoes, fresh mozzarella, and fragrant basil on a crispy, hand-stretched crust - a perfect vegetarian indulgence for Italian cuisine enthusiasts.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dish_summary(context, user_input, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define an assistant, which generates a json output and suggests whether the user is asking a general question or requesting recommendations, accordingly the search results will be shown or a general response will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assistant(context, user_input, llm):\n",
    "\n",
    "\n",
    "    assistant_prompt = [\n",
    "                SystemMessage(content=\"You are a helpful and knowledgeable assistant capable of providing food recommendations and answering general queries.\"),\n",
    "                HumanMessage(content=[\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f'''\n",
    "  Your task is to engage users in natural, friendly dialogue to understand their preferences, dietary restrictions, and culinary interests.\n",
    "Your goal is to summarize relevant food recommendations in two lines based on the user's inputs and the context if the user query is indicting that they want a recommendation. \n",
    "Otherwise you can simply request user to provide preferences such as which cuisine or dish they would like based on the context given. Do not answer if you don't have relevant knowledge about the query.\n",
    "\n",
    "Remember the context given is all the dishes we have.\n",
    "            \n",
    "User Input:\n",
    "\n",
    "{user_input}\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "\n",
    "The output should be strictly formatted in JSON, with the following structure:\n",
    "\"recommendation\": A field indicating whether a recommendation was made (\"yes\" or \"no\").\n",
    "\"response\": A text field containing the chatbot's conversational response to the user's input, including recommendations or additional questions if necessary.\n",
    "\"\n",
    "'''}])]\n",
    "    response = llm.invoke(assistant_prompt)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '''What all cusines do you have?'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test, if the user input is \"what all cuisines do you have?\" and somehow the enhanced search query has just given the context of Margharita pizza, the LLM should be able to judge that the user does not require recommendation but general response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"recommendation\": \"no\",\\n\"response\": \"I have information about an authentic Neapolitan-style Margherita pizza in my context. However, I don\\'t have details on the full range of cuisines available. Could you please specify which cuisine or type of dish you\\'re interested in so I can provide relevant recommendations?\"\\n}'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant(context, user_input, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! This works out perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quiz - 2**\n",
    "\n",
    "\n",
    "Did you finish the second Quiz yet? If not, complete it to test your knowledge?\n",
    "\n",
    "[Start Quiz 2](https://forms.gle/bE6jhuiGK67VvJUV8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Try it out**\n",
    "Here's an interactive exercise section that will guide you through various enhancements and deployments of the application. These tasks will help you deepen your understanding of the system and experiment with key components like data fetching, document reranking, and conversational flow with memory.\n",
    "\n",
    "1. Incorporate Data Fetching from S3\n",
    "All the necessary S3-related code is provided in the notebook. Your task is to create a pipeline that integrates this code into the app.py file, replacing the current local directory data fetching mechanism. This will involve modifying the sections of the app where data is loaded, ensuring that it fetches directly from your S3 bucket instead of a local file system. This step will make your application more scalable and cloud-ready.\n",
    "\n",
    "2. Document Reranking\n",
    "Explore the concept of document reranking within the app. Modify the search and retrieval process to incorporate reranking of search results based on relevance to the query. Implement a reranking mechanism that takes the initial search results and refines them further based on additional factors, such as user preferences or recent interaction history. This will help in delivering more accurate and relevant results. \n",
    "**Can we rerank on the basis of user inputs - \"high average rating\", \"low calorie\", \"price\", \"relevance\"?**\n",
    "\n",
    "3. Conversational Flow with Memory\n",
    "Enhance the conversational flow by adding memory to the chatbot. Implement a memory system that allows the chatbot to retain context from previous interactions and use this information in subsequent responses. This could involve tracking user preferences, past queries, or any important details that would help create a more personalized and coherent interaction flow.\n",
    "**If the user is asking a query again say pizza, do we again show the same recommendations (top 3 like before) or do explore other pizzas in the database that were not shown before?** -\n",
    "**Can we include a feedback loop?** Something to explore.\n",
    "\n",
    "4. Deploy the Application on EC2\n",
    "Follow the instructions provided in the README file to deploy the application on an Amazon EC2 instance. This task will involve setting up an EC2 instance, configuring the environment, and deploying your Streamlit application. This will allow you to run your application in the cloud, making it accessible from anywhere and providing a real-world experience in deploying applications on cloud infrastructure.\n",
    "Refer the following project - [Learn to Build an End-to-End Machine Learning Pipeline - Part 2](https://www.projectpro.io/project-use-case/build-and-deploy-an-end-to-end-machine-learning-pipeline-for-a-classification-model)\n",
    "\n",
    "By completing these exercises, you'll gain hands-on experience with cloud integration, search optimization, and conversational AI development, equipping you with the skills needed to build and deploy robust applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "In this project, we've explored the development and deployment of a conversational food recommendation assistant using Streamlit using Multimodal LLMs, with enhancements like image-based search, query rewriting, and integration with external data sources such as S3. The project demonstrates how to leverage advanced techniques for improving search results, while also focusing on building a flexible, interactive user interface. \n",
    "\n",
    "By completing the exercises, you'll gain valuable experience in cloud integration, Document Retreival tasks, and the development of AI-powered applications. These skills are critical in today's technology landscape, where personalization and efficiency are key to delivering impactful user experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interview Questions**\n",
    "\n",
    "\n",
    "To further solidify your understanding of the concepts taught in this project, here are some interview questions that you might encounter:\n",
    "\n",
    "* What is a multimodal LLM, and how does it differ from traditional text-based LLMs? Can you provide examples of use cases where multimodal LLMs are particularly effective?\n",
    "\n",
    "* In the context of multimodal LLMs, how can cross-modal retrieval systems be implemented?\n",
    "\n",
    "* Explain the purpose of using session state in a Streamlit application. How does it help maintain the conversational flow in a chatbot?\n",
    "\n",
    "* What is HyDE (Hypothetical Document Embeddings), and how does it improve search query performance? Provide an example scenario where using HyDE might be advantageous.\n",
    "\n",
    "* Discuss the importance of query rewriting in an LLM Application. How does it enhance the relevance of search results?\n",
    "\n",
    "* How can you integrate data fetching from an S3 bucket into a Streamlit application? What are the benefits of using cloud storage over a local file system?\n",
    "\n",
    "* Describe the concept of document reranking. How does reranking improve the quality of search results in a recommendation system?\n",
    "\n",
    "* What are the key considerations when deploying a Streamlit application on an EC2 instance? What challenges might you face, and how can you overcome them?\n",
    "\n",
    "* How does adding memory to a conversational chatbot improve user interaction? Can you describe a scenario where memory would be particularly useful?\n",
    "\n",
    "* Discuss the trade-offs between using specific versus broad search queries in a recommendation system. How does the system balance these to avoid irrelevant results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feedback**\n",
    "We'd love to hear your thoughts on this project! Please take a moment to fill out our feedback form and let us know how we can improve.\n",
    "\n",
    "[Feedback Form](https://forms.gle/YvLPCCLHzGb6HfpD6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
